{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f27e4f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "782e7528",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7p/pgzfcdb953s5cgb46bb6dv6h0000gn/T/ipykernel_45879/1526527318.py:1: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('nytaxi2022.csv')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VendorID</th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>tpep_dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>RatecodeID</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>extra</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>congestion_surcharge</th>\n",
       "      <th>airport_fee</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>01/01/2022 12:35:40 AM</td>\n",
       "      <td>01/01/2022 12:53:29 AM</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.80</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>142</td>\n",
       "      <td>236</td>\n",
       "      <td>1</td>\n",
       "      <td>14.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.65</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>21.95</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>01/01/2022 12:33:43 AM</td>\n",
       "      <td>01/01/2022 12:42:07 AM</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>236</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>13.30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>01/01/2022 12:53:21 AM</td>\n",
       "      <td>01/01/2022 01:02:19 AM</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.97</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>166</td>\n",
       "      <td>166</td>\n",
       "      <td>1</td>\n",
       "      <td>7.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.76</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>10.56</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>01/01/2022 12:25:21 AM</td>\n",
       "      <td>01/01/2022 12:35:23 AM</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.09</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>114</td>\n",
       "      <td>68</td>\n",
       "      <td>2</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>11.80</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>01/01/2022 12:36:48 AM</td>\n",
       "      <td>01/01/2022 01:14:20 AM</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.30</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>68</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>23.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>30.30</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   VendorID    tpep_pickup_datetime   tpep_dropoff_datetime  passenger_count  \\\n",
       "0         1  01/01/2022 12:35:40 AM  01/01/2022 12:53:29 AM              2.0   \n",
       "1         1  01/01/2022 12:33:43 AM  01/01/2022 12:42:07 AM              1.0   \n",
       "2         2  01/01/2022 12:53:21 AM  01/01/2022 01:02:19 AM              1.0   \n",
       "3         2  01/01/2022 12:25:21 AM  01/01/2022 12:35:23 AM              1.0   \n",
       "4         2  01/01/2022 12:36:48 AM  01/01/2022 01:14:20 AM              1.0   \n",
       "\n",
       "   trip_distance  RatecodeID store_and_fwd_flag  PULocationID  DOLocationID  \\\n",
       "0           3.80         1.0                  N           142           236   \n",
       "1           2.10         1.0                  N           236            42   \n",
       "2           0.97         1.0                  N           166           166   \n",
       "3           1.09         1.0                  N           114            68   \n",
       "4           4.30         1.0                  N            68           163   \n",
       "\n",
       "   payment_type  fare_amount  extra  mta_tax  tip_amount  tolls_amount  \\\n",
       "0             1         14.5    3.0      0.5        3.65           0.0   \n",
       "1             1          8.0    0.5      0.5        4.00           0.0   \n",
       "2             1          7.5    0.5      0.5        1.76           0.0   \n",
       "3             2          8.0    0.5      0.5        0.00           0.0   \n",
       "4             1         23.5    0.5      0.5        3.00           0.0   \n",
       "\n",
       "   improvement_surcharge  total_amount  congestion_surcharge  airport_fee  \n",
       "0                    0.3         21.95                   2.5          0.0  \n",
       "1                    0.3         13.30                   0.0          0.0  \n",
       "2                    0.3         10.56                   0.0          0.0  \n",
       "3                    0.3         11.80                   2.5          0.0  \n",
       "4                    0.3         30.30                   2.5          0.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('nytaxi2022.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b20e19b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39656098, 19)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87b118a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# convert \"\" or whitespace-only cells to NA so pandas counts them as missing\n",
    "df = df.replace(r\"^\\s*$\", pd.NA, regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b526e50",
   "metadata": {},
   "source": [
    "The code above just turns empty or whitespace-only strings into proper missing values. \n",
    "\n",
    "As CSV files often store \"empty\" cells as \"\" or spaces. By default, pandas won't treat \" \" as missing. Converting them to pd.NA makes your missingness checks and imputations work correctly. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6be6c39",
   "metadata": {},
   "source": [
    "# Analyzing and cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08d1a969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VendorID                       0\n",
       "tpep_pickup_datetime           0\n",
       "tpep_dropoff_datetime          0\n",
       "passenger_count          1368303\n",
       "trip_distance                  0\n",
       "RatecodeID               1368303\n",
       "store_and_fwd_flag       1368303\n",
       "PULocationID                   0\n",
       "DOLocationID                   0\n",
       "payment_type                   0\n",
       "fare_amount                    0\n",
       "extra                          0\n",
       "mta_tax                        0\n",
       "tip_amount                     0\n",
       "tolls_amount                   0\n",
       "improvement_surcharge          0\n",
       "total_amount                   0\n",
       "congestion_surcharge     1368303\n",
       "airport_fee              1368303\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of missing row within a columns\n",
    "df.isna().sum()            # per-column counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ac5a8cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.int64(1368303), np.float64(3.45))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = len(df)\n",
    "cols = [\"passenger_count\",\"RatecodeID\",\"store_and_fwd_flag\",\"congestion_surcharge\",\"airport_fee\"]\n",
    "\n",
    "miss_tbl = (\n",
    "    df[cols].isna().sum()\n",
    "      .to_frame(\"missing\")\n",
    "      .assign(pct=lambda t: (t[\"missing\"]/N*100).round(2))\n",
    ")\n",
    "miss_tbl  # quick table\n",
    "\n",
    "# Are the *same* rows missing all five?\n",
    "co_missing = df[cols].isna().all(axis=1)\n",
    "co_missing.sum(), (co_missing.mean()*100).round(2)  # count & %"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29488ad0",
   "metadata": {},
   "source": [
    "The code computes how much data is missing in a specific set of columns, then checks if it is the same rows missing across all of them. Firstly, it builds `miss_tbl`: for each column in [\"passenger_count\", \"RatecodeID\", \"store_and_fwd_flag\", \"congestion_surcharge\", \"airport_fee\"], it counts missing values and adds a percentage of total rows (N). Next, co_missing = df[cols].isna().all(axis=1) flags rows where all five of those columns are missing; co_missing.sum() gives the number of such rows and `co_missing.mean()*100` gives their percentage. \n",
    "This quickly shows both per-column gaps and whether there's a shared \"co-missing\" pattern-useful for deciding whether to impute together, drop those rows, or treat them specifically.\n",
    "\n",
    "Output confirms that the same 1,368,303 (~3.45%) are missing values in these columns: passenger_count, RatecodeID, store_and_fwd_flag, congestion_surcharge, airport_fee. Since co_missing.sum() is 1,368,303 and that matches the per-column missing counts you showed earlier, it means the same 1,368,303 rows are missing values in every one of those columns (≈3.45% of the dataset). There are no rows where only a subset of those columns is missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27f61aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keep:  ['tpep_pickup_datetime', 'tpep_dropoff_datetime', 'passenger_count', 'trip_distance', 'RatecodeID', 'PULocationID', 'DOLocationID', 'payment_type', 'extra']\n",
      "DROP (leakage):  ['fare_amount', 'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge', 'congestion_surcharge', 'airport_fee']\n",
      "DROP (constant):  []\n",
      "DROP (very missing):  []\n"
     ]
    }
   ],
   "source": [
    "TARGET = \"total_amount\"\n",
    "\n",
    "allowed = [\"tpep_pickup_datetime\",\"tpep_dropoff_datetime\",\n",
    "    \"passenger_count\",\"trip_distance\",\"RatecodeID\",\n",
    "    \"PULocationID\",\"DOLocationID\",\"payment_type\",\"extra\"]\n",
    "\n",
    "leakage = [\"fare_amount\",\"mta_tax\",\"tip_amount\",\"tolls_amount\",\n",
    "    \"improvement_surcharge\",\"congestion_surcharge\",\"airport_fee\", TARGET]\n",
    "\n",
    "present = [c for c in df.columns if c in allowed]\n",
    "leak_present = [c for c in df.columns if c in leakage and c != TARGET]\n",
    "\n",
    "# optional: remove near-constant and very-missing among the allowed set\n",
    "missing = df[present].isna().mean().sort_values(ascending=False)\n",
    "const_like = [c for c in present if df[c].nunique(dropna=True) <= 1]\n",
    "very_missing = [c for c in present if missing[c] > 0.40] #40% threshold; tweak\n",
    "\n",
    "keep = [c for c in present if c not in const_like and c not in very_missing]\n",
    "\n",
    "print(\"Keep: \", keep)\n",
    "print(\"DROP (leakage): \", leak_present)\n",
    "print(\"DROP (constant): \", const_like)\n",
    "print(\"DROP (very missing): \", very_missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e92f4e1",
   "metadata": {},
   "source": [
    "Builds a *clean feature list* and tells you what to keep vs drop, with guards against target leakage and junk columns. \n",
    "\n",
    "* TARGET = your label;\n",
    "- allowed = features you are allowed to use;\n",
    "- leakage = columns that directly compost `total_amount` (and must not be features)\n",
    "\n",
    "* present = intersection of allowed with the columns actually in df.\n",
    "* leak_present = any leakage columns that exist in df (to confirm whether the column is excluded)\n",
    "\n",
    "It then scores the present features:\n",
    "\n",
    "missing = percent missing per column,\n",
    "\n",
    "const_like = columns with ≤1 distinct non-NA value (no signal),\n",
    "\n",
    "very_missing = features with >40% missing (threshold you can tweak).\n",
    "\n",
    "keep = present minus (const_like ∪ very_missing). That’s your final whitelist."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea169b9a",
   "metadata": {},
   "source": [
    "## Engineered feature plus stabilisation before test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06e177b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7p/pgzfcdb953s5cgb46bb6dv6h0000gn/T/ipykernel_45879/2672286999.py:4: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  m[c] = pd.to_datetime(m[c], errors=\"coerce\", utc=True)\n",
      "/var/folders/7p/pgzfcdb953s5cgb46bb6dv6h0000gn/T/ipykernel_45879/2672286999.py:4: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  m[c] = pd.to_datetime(m[c], errors=\"coerce\", utc=True)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "m = df[keep + [TARGET]].copy()\n",
    "for c in [\"tpep_pickup_datetime\",\"tpep_dropoff_datetime\"]:\n",
    "    m[c] = pd.to_datetime(m[c], errors=\"coerce\", utc=True)\n",
    "\n",
    "m[\"trip_duration_min\"] = (m[\"tpep_dropoff_datetime\"] - m[\"tpep_pickup_datetime\"]).dt.total_seconds()/60\n",
    "m[\"pickup_hour\"] = m[\"tpep_pickup_datetime\"].dt.hour.astype(\"Int8\")\n",
    "m[\"pickup_dow\"]  = m[\"tpep_pickup_datetime\"].dt.dayofweek.astype(\"Int8\")\n",
    "m[\"pickup_month\"] = m[\"tpep_pickup_datetime\"].dt.month.astype(\"Int8\")   # <--- added\n",
    "\n",
    "# --- add stabilization step here ---\n",
    "m = m[(m[\"total_amount\"] > 0) &\n",
    "     (m[\"total_amount\"] < 300) &\n",
    "     (m[\"trip_distance\"] > 0) &\n",
    "     (m[\"trip_duration_min\"] > 0) &\n",
    "     (m[\"trip_duration_min\"] <= 180)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d35ea5b",
   "metadata": {},
   "source": [
    "NOTE: The stabilization step removes implausible fares (e.g. >$300), zero/negative distances,\n",
    "and unrealistic trip durations. Without it, extreme outliers (~$400k fares) \n",
    "would dominate RMSE and make the model look much worse than it actually is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa48cf37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df:  (39656098, 19)\n"
     ]
    }
   ],
   "source": [
    "print(\"df: \", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8954c8c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m.  (38795244, 14)\n"
     ]
    }
   ],
   "source": [
    "print(\"m. \", m.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a02f51c",
   "metadata": {},
   "source": [
    "## Sanity + fix dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c6b2feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure numerics are truly numeric (bad strings -> NaN)\n",
    "for c in [\"passenger_count\",\"trip_distance\",\"extra\",\"total_amount\"]:\n",
    "    m[c] = pd.to_numeric(m[c], errors=\"coerce\")\n",
    "\n",
    "# Keep engineered ints as floats so sklearn sees NaN correctly\n",
    "m[\"trip_duration_min\"] = m[\"trip_duration_min\"].astype(\"float32\")\n",
    "m[\"pickup_hour\"]       = m[\"pickup_hour\"].astype(\"float32\")\n",
    "m[\"pickup_dow\"]        = m[\"pickup_dow\"].astype(\"float32\")\n",
    "\n",
    "# add pickup_month (1–12)\n",
    "m[\"pickup_month\"]      = m[\"tpep_pickup_datetime\"].dt.month.astype(\"float32\")\n",
    "\n",
    "# Cast categoricals\n",
    "for c in [\"RatecodeID\",\"PULocationID\",\"DOLocationID\",\"payment_type\"]:\n",
    "    m[c] = m[c].astype(\"category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524b1ff7",
   "metadata": {},
   "source": [
    "## Random train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6dc1a7d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final X,y: (38795244, 11) (38795244,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "NUM = [\n",
    "    \"passenger_count\",\n",
    "    \"trip_distance\",\n",
    "    \"extra\",\n",
    "    \"trip_duration_min\",\n",
    "    \"pickup_hour\",\n",
    "    \"pickup_dow\",\n",
    "    \"pickup_month\"\n",
    "]\n",
    "CAT = [\"RatecodeID\", \"PULocationID\", \"DOLocationID\", \"payment_type\"]\n",
    "TARGET = \"total_amount\"\n",
    "\n",
    "X, y = m[NUM + CAT], m[TARGET].astype(\"float32\")\n",
    "print(\"final X,y:\", X.shape, y.shape)\n",
    "\n",
    "Xtr, Xte, ytr, yte = train_test_split(\n",
    "    X, y, test_size=0.30, random_state=42\n",
    ")\n",
    "\n",
    "# dropna for safety\n",
    "Xtr_clean = Xtr.dropna(); ytr_clean = ytr.loc[Xtr_clean.index]\n",
    "Xte_clean = Xte.dropna(); yte_clean = yte.loc[Xte_clean.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9cd0d3fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27156670, 11638574, 27156670, 11638574)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Xtr), len(Xte), len(ytr), len(yte)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d04211",
   "metadata": {},
   "source": [
    "## Sanity-check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ea323d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yte.describe(percentiles=[.95,.99,.999])\n",
    "(yte > 300).mean()   # share of crazy-high fares"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2ea8e2",
   "metadata": {},
   "source": [
    " ### 📝 Why I Changed the Encoding Strategy  \n",
    "\n",
    "Initially, I one-hot encoded **all categorical features**, including `PULocationID` and `DOLocationID`.  \n",
    "These columns have **hundreds of unique values**, so one-hot encoding created **tens of thousands of dummy columns**.  \n",
    "\n",
    "This caused two major issues:  \n",
    "- 🚨 **Feature explosion** → the transformed dataset became extremely wide, making Random Forest training unbearably slow (hours).  \n",
    "- 🚨 **Memory overhead** → every tree split had to scan across all those sparse dummy features, wasting CPU and RAM.  \n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Solution  \n",
    "\n",
    "- Keep **one-hot encoding (OHE)** only for **low-cardinality categoricals** (`RatecodeID`, `payment_type`).  \n",
    "- Pass through **high-cardinality features** (`PULocationID`, `DOLocationID`) as numeric integers instead of OHE.  \n",
    "\n",
    "---\n",
    "\n",
    "### 💡 Why this works  \n",
    "\n",
    "- 🌳 Tree-based models (Random Forest, LightGBM) handle numeric IDs directly and can split on them like categorical values.  \n",
    "- 📉 The feature space stays compact (a few dozen features instead of tens of thousands).  \n",
    "- ⚡ Training time drops from **hours → minutes** with no meaningful loss in predictive power.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78774f66",
   "metadata": {},
   "source": [
    "## Baseline Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ccdbb8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ridge] MAE=3.779  RMSE=7.263  R2=0.8094\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, MaxAbsScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Preprocessor (Ridge needs scaling + OHE)\n",
    "pre_ridge = ColumnTransformer([\n",
    "    (\"num\", Pipeline([\n",
    "        (\"sc\", MaxAbsScaler())\n",
    "    ]), NUM),\n",
    "    (\"cat_low\", OneHotEncoder(handle_unknown=\"ignore\", min_frequency=5), [\"RatecodeID\", \"payment_type\"]),\n",
    "    (\"cat_high\", \"passthrough\", [\"PULocationID\", \"DOLocationID\"]),\n",
    "], sparse_threshold=1.0)\n",
    "\n",
    "ridge = Pipeline([\n",
    "    (\"pre\", pre_ridge),\n",
    "    (\"est\", Ridge(alpha=1.0, solver=\"sag\", max_iter=2000, random_state=42))\n",
    "])\n",
    "\n",
    "ridge.fit(Xtr_clean, ytr_clean)\n",
    "pred = ridge.predict(Xte_clean)\n",
    "\n",
    "mae  = mean_absolute_error(yte_clean, pred)\n",
    "mse  = mean_squared_error(yte_clean, pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2   = r2_score(yte_clean, pred)\n",
    "\n",
    "print(f\"[Ridge] MAE={mae:.3f}  RMSE={rmse:.3f}  R2={r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dac0f1c",
   "metadata": {},
   "source": [
    "## Baseline Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "14d5317d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample predictions: [35.46774237 73.04246339 73.04246339 15.94616308 14.00170702]\n",
      "Sanity check complete ✅\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done   2 out of   5 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done   5 out of   5 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "# Sanity check - Random forest \n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# --- Minimal preprocessor (same structure as baseline) ---\n",
    "pre_rf = ColumnTransformer([\n",
    "    (\"num\", \"passthrough\", NUM),\n",
    "    (\"cat_low\", OneHotEncoder(handle_unknown=\"ignore\", min_frequency=5), [\"RatecodeID\", \"payment_type\"]),\n",
    "    (\"cat_high\", \"passthrough\", [\"PULocationID\", \"DOLocationID\"]),\n",
    "])\n",
    "\n",
    "# --- Tiny RF config for sanity check ---\n",
    "rf_debug = Pipeline([\n",
    "    (\"pre\", pre_rf),\n",
    "    (\"est\", RandomForestRegressor(\n",
    "        n_estimators=5,      # just 5 trees\n",
    "        max_depth=6,         # shallow trees\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        verbose=1\n",
    "    ))\n",
    "])\n",
    "\n",
    "# --- Small sample (e.g. 20k rows) ---\n",
    "Xtr_debug = Xtr_clean.sample(n=20_000, random_state=42)\n",
    "ytr_debug = ytr_clean.loc[Xtr_debug.index]\n",
    "\n",
    "rf_debug.fit(Xtr_debug, ytr_debug)\n",
    "pred_debug = rf_debug.predict(Xte_clean.head(5))  # predict just 5 rows\n",
    "\n",
    "print(\"Sample predictions:\", pred_debug)\n",
    "print(\"Sanity check complete ✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d39b42a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    1.9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RandomForest] MAE=1.461  RMSE=3.518  R2=0.9553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done  50 out of  50 | elapsed:    3.1s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Preprocessor (OHE only low-cardinality)\n",
    "pre_rf = ColumnTransformer([\n",
    "    (\"num\", \"passthrough\", NUM),\n",
    "    (\"cat_low\", OneHotEncoder(handle_unknown=\"ignore\", min_frequency=20), [\"RatecodeID\", \"payment_type\"]),\n",
    "    (\"cat_high\", \"passthrough\", [\"PULocationID\", \"DOLocationID\"]),  # <-- key change\n",
    "])\n",
    "\n",
    "rf = Pipeline([\n",
    "    (\"pre\", pre_rf),\n",
    "    (\"est\", RandomForestRegressor(\n",
    "        n_estimators=50,\n",
    "        max_depth=12,\n",
    "        min_samples_leaf=20,\n",
    "        max_features=0.5,\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        verbose=1\n",
    "    ))\n",
    "])\n",
    "\n",
    "Xtr_small = Xtr_clean.sample(n=200_000, random_state=42)  # smaller subsample for speed\n",
    "ytr_small = ytr_clean.loc[Xtr_small.index]\n",
    "\n",
    "rf.fit(Xtr_small, ytr_small)\n",
    "pred = rf.predict(Xte_clean)\n",
    "\n",
    "mae  = mean_absolute_error(yte_clean, pred)\n",
    "mse  = mean_squared_error(yte_clean, pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2   = r2_score(yte_clean, pred)\n",
    "\n",
    "print(f\"[RandomForest] MAE={mae:.3f}  RMSE={rmse:.3f}  R2={r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703c8bfe",
   "metadata": {},
   "source": [
    "## Baseline Lightgbm Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c60259a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5cccbf59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002943 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 956\n",
      "[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 21.468120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chris/Library/CloudStorage/Dropbox/work/NUS/DSA5208/Assignment_one/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] MAE=1.354  RMSE=3.311  R2=0.9604\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# --- Preprocessor ---\n",
    "pre_lgbm = ColumnTransformer([\n",
    "    (\"num\", \"passthrough\", NUM),\n",
    "    (\"cat_low\", OneHotEncoder(handle_unknown=\"ignore\", min_frequency=10), [\"RatecodeID\", \"payment_type\"]),\n",
    "    (\"cat_high\", \"passthrough\", [\"PULocationID\", \"DOLocationID\"]),\n",
    "], sparse_threshold=1.0)\n",
    "\n",
    "# --- Pipeline ---\n",
    "lgbm = Pipeline([\n",
    "    (\"pre\", pre_lgbm),\n",
    "    (\"est\", LGBMRegressor(\n",
    "        n_estimators=300,\n",
    "        learning_rate=0.05,\n",
    "        num_leaves=31,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# --- Train on subsample for speed ---\n",
    "Xtr_small = Xtr_clean.sample(n=200_000, random_state=42)\n",
    "ytr_small = ytr_clean.loc[Xtr_small.index]\n",
    "\n",
    "lgbm.fit(Xtr_small, ytr_small)\n",
    "\n",
    "pred = lgbm.predict(Xte_clean)\n",
    "\n",
    "mae  = mean_absolute_error(yte_clean, pred)\n",
    "mse  = mean_squared_error(yte_clean, pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2   = r2_score(yte_clean, pred)\n",
    "\n",
    "print(f\"[LightGBM] MAE={mae:.3f}  RMSE={rmse:.3f}  R2={r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861396b5",
   "metadata": {},
   "source": [
    "### 📊 Baseline Model Comparison  \n",
    "\n",
    "| Model        | MAE (↓) | RMSE (↓) | R² (↑) | Notes |\n",
    "|--------------|---------|----------|--------|-------|\n",
    "| **Ridge**    | 3.779   | 7.263    | 0.809  | Linear baseline, decent sanity check but underfits nonlinear patterns. |\n",
    "| **RandomForest** | 1.461   | 3.518    | 0.955  | Strong performance, captures nonlinearities, but slower to train. |\n",
    "| **LightGBM** | 1.354   | 3.311    | 0.960  | Best overall: fast, accurate, and efficient with large datasets. |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔎 Key Takeaways\n",
    "- Ridge provides a **baseline** but struggles with complex relationships.  \n",
    "- Random Forest captures more detail, but can be **computationally heavy**.  \n",
    "- LightGBM edges out Random Forest with **better accuracy and faster training**, making it the most practical baseline for large-scale experiments.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d22bd20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.9 (pyenv)",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
