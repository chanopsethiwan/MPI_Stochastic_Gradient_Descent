{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e937e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "938fe040",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5n/m32d0hsx4959r42x3mssyjj40000gn/T/ipykernel_79682/1526527318.py:1: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('nytaxi2022.csv')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VendorID</th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>tpep_dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>RatecodeID</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>extra</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>congestion_surcharge</th>\n",
       "      <th>airport_fee</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>01/01/2022 12:35:40 AM</td>\n",
       "      <td>01/01/2022 12:53:29 AM</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.80</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>142</td>\n",
       "      <td>236</td>\n",
       "      <td>1</td>\n",
       "      <td>14.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.65</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>21.95</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>01/01/2022 12:33:43 AM</td>\n",
       "      <td>01/01/2022 12:42:07 AM</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>236</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>13.30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>01/01/2022 12:53:21 AM</td>\n",
       "      <td>01/01/2022 01:02:19 AM</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.97</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>166</td>\n",
       "      <td>166</td>\n",
       "      <td>1</td>\n",
       "      <td>7.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.76</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>10.56</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>01/01/2022 12:25:21 AM</td>\n",
       "      <td>01/01/2022 12:35:23 AM</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.09</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>114</td>\n",
       "      <td>68</td>\n",
       "      <td>2</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>11.80</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>01/01/2022 12:36:48 AM</td>\n",
       "      <td>01/01/2022 01:14:20 AM</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.30</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>68</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>23.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>30.30</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   VendorID    tpep_pickup_datetime   tpep_dropoff_datetime  passenger_count  \\\n",
       "0         1  01/01/2022 12:35:40 AM  01/01/2022 12:53:29 AM              2.0   \n",
       "1         1  01/01/2022 12:33:43 AM  01/01/2022 12:42:07 AM              1.0   \n",
       "2         2  01/01/2022 12:53:21 AM  01/01/2022 01:02:19 AM              1.0   \n",
       "3         2  01/01/2022 12:25:21 AM  01/01/2022 12:35:23 AM              1.0   \n",
       "4         2  01/01/2022 12:36:48 AM  01/01/2022 01:14:20 AM              1.0   \n",
       "\n",
       "   trip_distance  RatecodeID store_and_fwd_flag  PULocationID  DOLocationID  \\\n",
       "0           3.80         1.0                  N           142           236   \n",
       "1           2.10         1.0                  N           236            42   \n",
       "2           0.97         1.0                  N           166           166   \n",
       "3           1.09         1.0                  N           114            68   \n",
       "4           4.30         1.0                  N            68           163   \n",
       "\n",
       "   payment_type  fare_amount  extra  mta_tax  tip_amount  tolls_amount  \\\n",
       "0             1         14.5    3.0      0.5        3.65           0.0   \n",
       "1             1          8.0    0.5      0.5        4.00           0.0   \n",
       "2             1          7.5    0.5      0.5        1.76           0.0   \n",
       "3             2          8.0    0.5      0.5        0.00           0.0   \n",
       "4             1         23.5    0.5      0.5        3.00           0.0   \n",
       "\n",
       "   improvement_surcharge  total_amount  congestion_surcharge  airport_fee  \n",
       "0                    0.3         21.95                   2.5          0.0  \n",
       "1                    0.3         13.30                   0.0          0.0  \n",
       "2                    0.3         10.56                   0.0          0.0  \n",
       "3                    0.3         11.80                   2.5          0.0  \n",
       "4                    0.3         30.30                   2.5          0.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('nytaxi2022.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9887dec6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39656098, 19)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef86a34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# convert \"\" or whitespace-only cells to NA so pandas counts them as missing\n",
    "df = df.replace(r\"^\\s*$\", pd.NA, regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d254bd",
   "metadata": {},
   "source": [
    "The code above just turns empty or whitespace-only strings into proper missing values. \n",
    "\n",
    "* As CSV files often store \"empty\" cells as \"\" or spaces. By default, pandas won't treat \" \" as missing. Converting them to pd.NA makes your missingness checks and imputations work correctly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3236d3f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VendorID                       0\n",
       "tpep_pickup_datetime           0\n",
       "tpep_dropoff_datetime          0\n",
       "passenger_count          1368303\n",
       "trip_distance                  0\n",
       "RatecodeID               1368303\n",
       "store_and_fwd_flag       1368303\n",
       "PULocationID                   0\n",
       "DOLocationID                   0\n",
       "payment_type                   0\n",
       "fare_amount                    0\n",
       "extra                          0\n",
       "mta_tax                        0\n",
       "tip_amount                     0\n",
       "tolls_amount                   0\n",
       "improvement_surcharge          0\n",
       "total_amount                   0\n",
       "congestion_surcharge     1368303\n",
       "airport_fee              1368303\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of missing row within a columns\n",
    "df.isna().sum()            # per-column counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "021f9803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.int64(1368303), np.float64(3.45))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = len(df)\n",
    "cols = [\"passenger_count\",\"RatecodeID\",\"store_and_fwd_flag\",\"congestion_surcharge\",\"airport_fee\"]\n",
    "\n",
    "miss_tbl = (\n",
    "    df[cols].isna().sum()\n",
    "      .to_frame(\"missing\")\n",
    "      .assign(pct=lambda t: (t[\"missing\"]/N*100).round(2))\n",
    ")\n",
    "miss_tbl  # quick table\n",
    "\n",
    "# Are the *same* rows missing all five?\n",
    "co_missing = df[cols].isna().all(axis=1)\n",
    "co_missing.sum(), (co_missing.mean()*100).round(2)  # count & %"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af61f5f1",
   "metadata": {},
   "source": [
    "The code computes how much data is missing in a specific set of columns, then checks if it is the same rows missing across all of them. Firstly, it builds `miss_tbl`: for each column in [\"passenger_count\", \"RatecodeID\", \"store_and_fwd_flag\", \"congestion_surcharge\", \"airport_fee\"], it counts missing values and adds a percentage of total rows (N). Next, co_missing = df[cols].isna().all(axis=1) flags rows where all five of those columns are missing; co_missing.sum() gives the number of such rows and `co_missing.mean()*100` gives their percentage. \n",
    "This quickly shows both per-column gaps and whether there's a shared \"co-missing\" pattern-useful for deciding whether to impute together, drop those rows, or treat them specifically.\n",
    "\n",
    "Output confirms that the same 1,368,303 (~3.45%) are missing values in these columns: passenger_count, RatecodeID, store_and_fwd_flag, congestion_surcharge, airport_fee. Since co_missing.sum() is 1,368,303 and that matches the per-column missing counts you showed earlier, it means the same 1,368,303 rows are missing values in every one of those columns (≈3.45% of the dataset). There are no rows where only a subset of those columns is missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa2ec5f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keep:  ['tpep_pickup_datetime', 'tpep_dropoff_datetime', 'passenger_count', 'trip_distance', 'RatecodeID', 'PULocationID', 'DOLocationID', 'payment_type', 'extra']\n",
      "DROP (leakage):  ['fare_amount', 'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge', 'congestion_surcharge', 'airport_fee']\n",
      "DROP (constant):  []\n",
      "DROP (very missing):  []\n"
     ]
    }
   ],
   "source": [
    "TARGET = \"total_amount\"\n",
    "\n",
    "allowed = [\"tpep_pickup_datetime\",\"tpep_dropoff_datetime\",\n",
    "    \"passenger_count\",\"trip_distance\",\"RatecodeID\",\n",
    "    \"PULocationID\",\"DOLocationID\",\"payment_type\",\"extra\"]\n",
    "\n",
    "leakage = [\"fare_amount\",\"mta_tax\",\"tip_amount\",\"tolls_amount\",\n",
    "    \"improvement_surcharge\",\"congestion_surcharge\",\"airport_fee\", TARGET]\n",
    "\n",
    "present = [c for c in df.columns if c in allowed]\n",
    "leak_present = [c for c in df.columns if c in leakage and c != TARGET]\n",
    "\n",
    "# optional: remove near-constant and very-missing among the allowed set\n",
    "missing = df[present].isna().mean().sort_values(ascending=False)\n",
    "const_like = [c for c in present if df[c].nunique(dropna=True) <= 1]\n",
    "very_missing = [c for c in present if missing[c] > 0.40] #40% threshold; tweak\n",
    "\n",
    "keep = [c for c in present if c not in const_like and c not in very_missing]\n",
    "\n",
    "print(\"Keep: \", keep)\n",
    "print(\"DROP (leakage): \", leak_present)\n",
    "print(\"DROP (constant): \", const_like)\n",
    "print(\"DROP (very missing): \", very_missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8ab601",
   "metadata": {},
   "source": [
    "Builds a *clean feature list* and tells you what to keep vs drop, with guards against target leakage and junk columns. \n",
    "\n",
    "* TARGET = your label;\n",
    "- allowed = features you are allowed to use;\n",
    "- leakage = columns that directly compost `total_amount` (and must not be features)\n",
    "\n",
    "* present = intersection of allowed with the columns actually in df.\n",
    "* leak_present = any leakage columns that exist in df (to confirm whether the column is excluded)\n",
    "\n",
    "It then scores the present features:\n",
    "\n",
    "missing = percent missing per column,\n",
    "\n",
    "const_like = columns with ≤1 distinct non-NA value (no signal),\n",
    "\n",
    "very_missing = features with >40% missing (threshold you can tweak).\n",
    "\n",
    "keep = present minus (const_like ∪ very_missing). That’s your final whitelist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3969d62b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5n/m32d0hsx4959r42x3mssyjj40000gn/T/ipykernel_79682/3375058208.py:4: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  m[c] = pd.to_datetime(m[c], errors=\"coerce\", utc=True)\n",
      "/var/folders/5n/m32d0hsx4959r42x3mssyjj40000gn/T/ipykernel_79682/3375058208.py:4: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  m[c] = pd.to_datetime(m[c], errors=\"coerce\", utc=True)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "m = df[keep + [TARGET]].copy()\n",
    "for c in [\"tpep_pickup_datetime\",\"tpep_dropoff_datetime\"]:\n",
    "    m[c] = pd.to_datetime(m[c], errors=\"coerce\", utc=True)\n",
    "\n",
    "m[\"trip_duration_min\"] = (m[\"tpep_dropoff_datetime\"] - m[\"tpep_pickup_datetime\"]).dt.total_seconds()/60\n",
    "m[\"pickup_hour\"] = m[\"tpep_pickup_datetime\"].dt.hour.astype(\"Int8\")\n",
    "m[\"pickup_dow\"]  = m[\"tpep_pickup_datetime\"].dt.dayofweek.astype(\"Int8\")\n",
    "m[\"pickup_month\"]  = m[\"tpep_pickup_datetime\"].dt.month.astype(\"Int8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53e7a8fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df:  (39656098, 19)\n"
     ]
    }
   ],
   "source": [
    "print(\"df: \", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a361a515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m.  (39656098, 13)\n"
     ]
    }
   ],
   "source": [
    "print(\"m. \", m.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da6cf64",
   "metadata": {},
   "source": [
    "# Sanity + fix dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6af68c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df: (39656098, 19)\n",
      "m: (39656098, 13)\n"
     ]
    }
   ],
   "source": [
    "print(\"df:\", df.shape)                 # should be ~ (39656098, 19)\n",
    "print(\"m:\",  m.shape)                  # should be (same rows, ~13 cols)\n",
    "\n",
    "# If m shows 0 rows here, your df got filtered earlier — re-read the CSV\n",
    "# df = pd.read_csv(\"nytaxi2022.csv\", low_memory=False)\n",
    "\n",
    "# Make sure numerics are truly numeric (bad strings -> NaN)\n",
    "for c in [\"passenger_count\",\"trip_distance\",\"extra\",\"total_amount\"]:\n",
    "    m[c] = pd.to_numeric(m[c], errors=\"coerce\")\n",
    "\n",
    "# Keep engineered ints as floats so sklearn sees NaN correctly\n",
    "m[\"trip_duration_min\"] = m[\"trip_duration_min\"].astype(\"float32\")\n",
    "m[\"pickup_hour\"]       = m[\"pickup_hour\"].astype(\"float32\")\n",
    "m[\"pickup_dow\"]        = m[\"pickup_dow\"].astype(\"float32\")\n",
    "\n",
    "# Cast categoricals\n",
    "for c in [\"RatecodeID\",\"PULocationID\",\"DOLocationID\",\"payment_type\"]:\n",
    "    m[c] = m[c].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "52d0af23",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.to_parquet(\"data/nytaxi2022_cleaned.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "72722cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.to_csv(\"data/nytaxi2022_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a5b7de",
   "metadata": {},
   "source": [
    "# random train/test split and keep all learned preprocessing inside a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6a5fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final X,y: (39656098, 10) (39656098,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "NUM = [\"passenger_count\",\"trip_distance\",\"extra\",\"trip_duration_min\",\"pickup_hour\",\"pickup_dow\"]\n",
    "CAT = [\"RatecodeID\",\"PULocationID\",\"DOLocationID\",\"payment_type\"]\n",
    "TARGET = \"total_amount\"\n",
    "\n",
    "X, y = m[NUM + CAT], m[TARGET].astype(\"float32\")\n",
    "print(\"final X,y:\", X.shape, y.shape)   # must show non-zero rows\n",
    "\n",
    "Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4450212f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27759268, 11896830, 27759268, 11896830)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Xtr), len(Xte), len(ytr), len(yte)\n",
    "# expected ≈ (27,759,268, 11,896,830, 27,759,268, 11,896,830)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eebfd88e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "passenger_count      958331\n",
       "trip_distance             0\n",
       "extra                     0\n",
       "trip_duration_min         0\n",
       "pickup_hour               0\n",
       "pickup_dow                0\n",
       "RatecodeID           958331\n",
       "PULocationID              0\n",
       "DOLocationID              0\n",
       "payment_type              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtr.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2379223",
   "metadata": {},
   "source": [
    "### Baseline Ridge-Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1b8208a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ridge] MAE=5.872  RMSE=16.987  R2=0.4314\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, MaxAbsScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# quick knobs for speed; set to None to use all rows\n",
    "N_TRAIN = 1_000_000\n",
    "N_TEST  = 500_000\n",
    "\n",
    "# preprocessing (fit on train only inside pipeline)\n",
    "pre_ridge = ColumnTransformer([\n",
    "    (\"num\", Pipeline([\n",
    "        (\"imp\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"sc\",  MaxAbsScaler())          # keeps sparse matrix fast\n",
    "    ]), NUM),\n",
    "    (\"cat\", Pipeline([\n",
    "        (\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"oh\",  OneHotEncoder(handle_unknown=\"ignore\", min_frequency=5))\n",
    "    ]), CAT),\n",
    "], sparse_threshold=1.0)\n",
    "\n",
    "ridge = Pipeline([\n",
    "    (\"pre\", pre_ridge),\n",
    "    (\"est\", Ridge(alpha=1.0, solver=\"sag\", max_iter=2000, random_state=42))\n",
    "])\n",
    "\n",
    "# sample for faster iteration\n",
    "idx_tr = Xtr.sample(n=N_TRAIN, random_state=42).index if N_TRAIN else Xtr.index\n",
    "idx_te = Xte.sample(n=N_TEST,  random_state=42).index if N_TEST  else Xte.index\n",
    "\n",
    "ridge.fit(Xtr.loc[idx_tr], ytr.loc[idx_tr])\n",
    "pred = ridge.predict(Xte.loc[idx_te])\n",
    "\n",
    "mae = mean_absolute_error(yte.loc[idx_te], pred)\n",
    "try:\n",
    "    rmse = mean_squared_error(yte.loc[idx_te], pred, squared=False)  # newer sklearn\n",
    "except TypeError:\n",
    "    rmse = np.sqrt(mean_squared_error(yte.loc[idx_te], pred))        # older sklearn\n",
    "r2  = r2_score(yte.loc[idx_te], pred)\n",
    "print(f\"[Ridge] MAE={mae:.3f}  RMSE={rmse:.3f}  R2={r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a6b2fc",
   "metadata": {},
   "source": [
    "### Baseline Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc91684f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed: 10.2min\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed: 10.8min finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:    1.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RandomForest] MAE=1.740  RMSE=14.881  R2=0.5637\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# reuse N_TRAIN / N_TEST from above\n",
    "\n",
    "pre_tree = ColumnTransformer([\n",
    "    (\"num\", SimpleImputer(strategy=\"median\"), NUM),\n",
    "    (\"cat\", Pipeline([\n",
    "        (\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"oh\",  OneHotEncoder(handle_unknown=\"ignore\", min_frequency=20))  # collapse rares a bit more\n",
    "    ]), CAT),\n",
    "])\n",
    "\n",
    "rf = Pipeline([\n",
    "    (\"pre\", pre_tree),\n",
    "    (\"est\", RandomForestRegressor(\n",
    "        n_estimators=200,\n",
    "        max_depth=16,\n",
    "        min_samples_leaf=10,\n",
    "        max_features=0.5,\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        verbose=1\n",
    "    ))\n",
    "])\n",
    "\n",
    "idx_tr = Xtr.sample(n=N_TRAIN, random_state=42).index if N_TRAIN else Xtr.index\n",
    "idx_te = Xte.sample(n=N_TEST,  random_state=42).index if N_TEST  else Xte.index\n",
    "\n",
    "rf.fit(Xtr.loc[idx_tr], ytr.loc[idx_tr])\n",
    "pred = rf.predict(Xte.loc[idx_te])\n",
    "\n",
    "mae = mean_absolute_error(yte.loc[idx_te], pred)\n",
    "try:\n",
    "    rmse = mean_squared_error(yte.loc[idx_te], pred, squared=False)\n",
    "except TypeError:\n",
    "    rmse = np.sqrt(mean_squared_error(yte.loc[idx_te], pred))\n",
    "r2  = r2_score(yte.loc[idx_te], pred)\n",
    "print(f\"[RandomForest] MAE={mae:.3f}  RMSE={rmse:.3f}  R2={r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9d171be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n= 200 | OOB R2=0.7778 | Test R2=0.5637 | MAE=1.740 | RMSE=14.881 | fit=722.0s\n",
      "n= 400 | OOB R2=0.7778 | Test R2=0.5637 | MAE=1.738 | RMSE=14.880 | fit=1333.1s\n",
      "n= 800 | OOB R2=0.7781 | Test R2=0.5638 | MAE=1.737 | RMSE=14.878 | fit=2834.6s\n",
      "\n",
      "Best so far: n=800 | Test R2=0.5638 | OOB R2=0.7781\n"
     ]
    }
   ],
   "source": [
    "# --- Step 1: grow trees (keep other params fixed) ---\n",
    "from time import perf_counter\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Assumes you already have: Xtr, Xte, ytr, yte, NUM, CAT\n",
    "\n",
    "# Build preprocessor if not already defined\n",
    "if 'pre_tree' not in globals():\n",
    "    pre_tree = ColumnTransformer([\n",
    "        (\"num\", SimpleImputer(strategy=\"median\"), NUM),\n",
    "        (\"cat\", Pipeline([\n",
    "            (\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "            (\"oh\",  OneHotEncoder(handle_unknown=\"ignore\", min_frequency=20))\n",
    "        ]), CAT),\n",
    "    ])\n",
    "\n",
    "# Use a sample for speed while testing (adjust or set to full later)\n",
    "N_TRAIN = 1_000_000\n",
    "N_TEST  = 500_000\n",
    "if 'idx_tr' not in globals():\n",
    "    idx_tr = Xtr.sample(n=min(N_TRAIN, len(Xtr)), random_state=42).index\n",
    "if 'idx_te' not in globals():\n",
    "    idx_te = Xte.sample(n=min(N_TEST, len(Xte)), random_state=42).index\n",
    "\n",
    "def _rmse(y_true, y_pred):\n",
    "    try:\n",
    "        return mean_squared_error(y_true, y_pred, squared=False)\n",
    "    except TypeError:\n",
    "        return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "def try_n(n_estimators):\n",
    "    model = Pipeline([\n",
    "        (\"pre\", pre_tree),\n",
    "        (\"est\", RandomForestRegressor(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=16,\n",
    "            min_samples_leaf=10,\n",
    "            max_features=0.5,\n",
    "            bootstrap=True, oob_score=True,\n",
    "            n_jobs=-1, random_state=42, verbose=0\n",
    "        ))\n",
    "    ])\n",
    "    t0 = perf_counter(); model.fit(Xtr.loc[idx_tr], ytr.loc[idx_tr]); fit_s = perf_counter() - t0\n",
    "    pred = model.predict(Xte.loc[idx_te])\n",
    "    oob = model.named_steps[\"est\"].oob_score_\n",
    "    r2  = r2_score(yte.loc[idx_te], pred)\n",
    "    mae = mean_absolute_error(yte.loc[idx_te], pred)\n",
    "    rmse = _rmse(yte.loc[idx_te], pred)\n",
    "    print(f\"n={n_estimators:>4} | OOB R2={oob:.4f} | Test R2={r2:.4f} | MAE={mae:.3f} | RMSE={rmse:.3f} | fit={fit_s:.1f}s\")\n",
    "    return {\"n_estimators\": n_estimators, \"oob_r2\": oob, \"test_r2\": r2,\n",
    "            \"test_mae\": mae, \"test_rmse\": rmse, \"fit_s\": round(fit_s,2), \"model\": model}\n",
    "\n",
    "results = [try_n(n) for n in (200, 400, 800)]\n",
    "\n",
    "# Keep the best by Test R²\n",
    "best = max(results, key=lambda d: d[\"test_r2\"])\n",
    "best_n = best[\"n_estimators\"]; best_model = best[\"model\"]\n",
    "print(f\"\\nBest so far: n={best_n} | Test R2={best['test_r2']:.4f} | OOB R2={best['oob_r2']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d035fdca",
   "metadata": {},
   "source": [
    "Changing the number of tree (n_estimators) and keep everything else fixed to see if more trees improve accuracy (OOB/Test R^2) and by how much.\n",
    "\n",
    "Test R² barely changes from 200→800 trees (0.5637 → 0.5638), while fit time ~4×.\n",
    "\n",
    "OOB R² is also flat (~0.778).\n",
    "\n",
    "➡️ Already variance-limited; adding trees won’t help much. keep 200 (for speed) or 400 (final) and tune leaf size / depth / max_features next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6c28591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 5, 0.5, 0.5652632225083358, 0.7792799752766375)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# uses: pre_tree, Xtr, Xte, ytr, yte\n",
    "from time import perf_counter\n",
    "import numpy as np, pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# sample for speed (reuse your idx_tr/idx_te if already defined)\n",
    "N_TRAIN, N_TEST = 1_000_000, 500_000\n",
    "if 'idx_tr' not in globals(): idx_tr = Xtr.sample(n=min(N_TRAIN, len(Xtr)), random_state=42).index\n",
    "if 'idx_te' not in globals(): idx_te = Xte.sample(n=min(N_TEST, len(Xte)),  random_state=42).index\n",
    "\n",
    "def _rmse(a,b):\n",
    "    try: from sklearn.metrics import mean_squared_error as mse; return mse(a,b,squared=False)\n",
    "    except TypeError: from sklearn.metrics import mean_squared_error as mse; import numpy as np; return np.sqrt(mse(a,b))\n",
    "\n",
    "def try_cfg(max_depth, min_leaf, max_feats, n_estimators=400):\n",
    "    model = Pipeline([\n",
    "        (\"pre\", pre_tree),\n",
    "        (\"est\", RandomForestRegressor(\n",
    "            n_estimators=n_estimators, max_depth=max_depth,\n",
    "            min_samples_leaf=min_leaf, max_features=max_feats,\n",
    "            bootstrap=True, oob_score=True, n_jobs=-1, random_state=42, verbose=0\n",
    "        ))\n",
    "    ])\n",
    "    t0=perf_counter(); model.fit(Xtr.loc[idx_tr], ytr.loc[idx_tr]); t=perf_counter()-t0\n",
    "    pred = model.predict(Xte.loc[idx_te])\n",
    "    return {\n",
    "        \"max_depth\":max_depth, \"min_leaf\":min_leaf, \"max_feats\":max_feats,\n",
    "        \"oob_r2\":model.named_steps[\"est\"].oob_score_,\n",
    "        \"test_r2\":r2_score(yte.loc[idx_te], pred),\n",
    "        \"mae\":mean_absolute_error(yte.loc[idx_te], pred),\n",
    "        \"rmse\":_rmse(yte.loc[idx_te], pred),\n",
    "        \"fit_s\":round(t,1), \"model\":model\n",
    "    }\n",
    "\n",
    "cands = [(16,10,0.5),(16,5,0.5),(20,10,0.5),(20,5,0.5),(16,10,0.8),(20,5,0.8)]\n",
    "rows = [try_cfg(*p) for p in cands]\n",
    "pd.DataFrame([{k:v for k,v in r.items() if k!=\"model\"} for r in rows]).sort_values(\"test_r2\", ascending=False)\n",
    "best = max(rows, key=lambda r: r[\"test_r2\"])\n",
    "best[\"max_depth\"], best[\"min_leaf\"], best[\"max_feats\"], best[\"test_r2\"], best[\"oob_r2\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70070f1c",
   "metadata": {},
   "source": [
    "The code above is a tiny hyperparameter sweep for Random Forest that keep preprocessing inside the pipeline (no leakage), uses a big but manageable sample for speed, and reports both OOB (bagging's internal estimate) and held-out Test metrics to choose the best bias/variance trade-off before training anything huge\n",
    "\n",
    "`max_depth` => deeper trees fit more detail (lower bias, higher variance (slower))\n",
    "`min_sample_leaf` => Smaller leaves allow purer leaves (lower bias, higher variance)\n",
    "`max_features` => more features considered per split (trees are more correlated and complex) -> lower bias, higher variance (and slower). Smaller values de-correlate trees and can improve generalization\n",
    "\n",
    "`test_r2` => r^2 on your held-out test slice. Good standard - optimize for this\n",
    "`oob_r^2` => out-of-bag computed R^2 computed from the training bootstrap samples. It's a fast, internal estimate; good for trend checking, but the test R^2 win when results disagree \n",
    "\n",
    "`fit_s` => wall-clock training time. Use it as a tiebreaker-prefer the fastest config within a hair of the best R^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335545c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid's rmse: 5.91303\n",
      "[200]\tvalid's rmse: 5.90146\n",
      "Early stopping, best iteration is:\n",
      "[140]\tvalid's rmse: 5.88559\n",
      "Best iteration: 140\n",
      "Best valid RMSE: 5.885585734779989\n",
      "[LightGBM] MAE=1.732  RMSE=116.728  R2=0.0206\n"
     ]
    }
   ],
   "source": [
    "# pip install -U lightgbm   # macOS once: brew install libomp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "NUM = [\"passenger_count\",\"trip_distance\",\"extra\",\"trip_duration_min\",\"pickup_hour\",\"pickup_dow\"]\n",
    "CAT = [\"RatecodeID\",\"PULocationID\",\"DOLocationID\",\"payment_type\"]\n",
    "FEATS = NUM + CAT\n",
    "\n",
    "# Dtypes (very important for LGBM)\n",
    "Xtr = Xtr.copy(); Xte = Xte.copy()\n",
    "for c in CAT:\n",
    "    Xtr[c] = Xtr[c].astype(\"category\")\n",
    "    Xte[c] = Xte[c].astype(\"category\")\n",
    "for c in NUM:\n",
    "    Xtr[c] = pd.to_numeric(Xtr[c], errors=\"coerce\")\n",
    "    Xte[c] = pd.to_numeric(Xte[c], errors=\"coerce\")\n",
    "\n",
    "# Build train/valid from TRAIN ONLY\n",
    "N_TRAIN, N_VALID = 1_000_000, 300_000\n",
    "chunk = Xtr.sample(n=min(N_TRAIN+N_VALID, len(Xtr)), random_state=42)\n",
    "idx_tr = chunk.sample(n=min(N_TRAIN, len(chunk)), random_state=42).index\n",
    "idx_va = chunk.drop(idx_tr).index\n",
    "\n",
    "train_ds = lgb.Dataset(Xtr.loc[idx_tr, FEATS], label=ytr.loc[idx_tr], categorical_feature=CAT, free_raw_data=False)\n",
    "valid_ds = lgb.Dataset(Xtr.loc[idx_va, FEATS], label=ytr.loc[idx_va], categorical_feature=CAT, free_raw_data=False)\n",
    "\n",
    "params = dict(\n",
    "    objective=\"regression\",\n",
    "    metric=\"rmse\",\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=64,\n",
    "    feature_fraction=0.8,   # colsample_bytree\n",
    "    bagging_fraction=0.8,   # subsample\n",
    "    bagging_freq=1,\n",
    "    min_data_in_leaf=100,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "# NOTE: use callbacks for early stopping & logging\n",
    "gbm = lgb.train(\n",
    "    params,\n",
    "    train_ds,\n",
    "    num_boost_round=5000,\n",
    "    valid_sets=[valid_ds],\n",
    "    valid_names=[\"valid\"],\n",
    "    callbacks=[lgb.early_stopping(100), lgb.log_evaluation(100)]\n",
    ")\n",
    "\n",
    "print(\"Best iteration:\", gbm.best_iteration)\n",
    "print(\"Best valid RMSE:\", gbm.best_score[\"valid\"][\"rmse\"])\n",
    "\n",
    "# Evaluate on the FULL TEST set\n",
    "pred = gbm.predict(Xte[FEATS], num_iteration=gbm.best_iteration)\n",
    "try:\n",
    "    rmse = mean_squared_error(yte, pred, squared=False)\n",
    "except TypeError:\n",
    "    rmse = np.sqrt(mean_squared_error(yte, pred))\n",
    "mae = mean_absolute_error(yte, pred)\n",
    "r2  = r2_score(yte, pred)\n",
    "print(f\"[LightGBM] MAE={mae:.3f}  RMSE={rmse:.3f}  R2={r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cbaef53f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trim keep %: 99.12651521455716\n",
      "MAE (trimmed): 15.526301938516246\n",
      "RMSE(trimmed): 24.320588348923422\n",
      "R2  (trimmed): -0.9449624630175439\n",
      "RMSE(clipped): 24.42364182107185\n",
      "R2  (clipped): -0.8489099367166202\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "\n",
    "def rmse_safe(y_true, y_pred):\n",
    "    try: return mean_squared_error(y_true, y_pred, squared=False)\n",
    "    except TypeError: return (mean_squared_error(y_true, y_pred))**0.5\n",
    "\n",
    "# pick an upper cap *from training only* (no leakage)\n",
    "cap = ytr.quantile(0.999)  # e.g., ~ $200–$300; adjust if you like\n",
    "\n",
    "# evaluate on a trimmed TEST (same rule applied)\n",
    "mask_trim = (yte >= 0) & (yte <= cap)\n",
    "print(\"Trim keep %:\", float(mask_trim.mean())*100)\n",
    "\n",
    "pred_trim = pred[mask_trim]\n",
    "yte_trim  = yte[mask_trim]\n",
    "\n",
    "print(\"MAE (trimmed):\", mean_absolute_error(yte_trim, pred_trim))\n",
    "print(\"RMSE(trimmed):\", rmse_safe(yte_trim, pred_trim))\n",
    "print(\"R2  (trimmed):\", r2_score(yte_trim, pred_trim))\n",
    "\n",
    "# optional: clipped version instead of dropping\n",
    "yte_clip  = yte.clip(lower=0, upper=cap)\n",
    "pred_clip = np.clip(pred, 0, cap)\n",
    "print(\"RMSE(clipped):\", rmse_safe(yte_clip, pred_clip))\n",
    "print(\"R2  (clipped):\", r2_score(yte_clip, pred_clip))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5ccac4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from TRAIN ONLY, choose a target cap (no leakage)\n",
    "y_cap = ytr.quantile(0.999)\n",
    "\n",
    "mask_tr = (\n",
    "    (Xtr[\"trip_distance\"] > 0) &\n",
    "    (Xtr[\"trip_duration_min\"] > 0) & (Xtr[\"trip_duration_min\"] <= 180) &\n",
    "    (ytr >= 0) & (ytr <= y_cap)\n",
    ")\n",
    "mask_te = (\n",
    "    (Xte[\"trip_distance\"] > 0) &\n",
    "    (Xte[\"trip_duration_min\"] > 0) & (Xte[\"trip_duration_min\"] <= 180) &\n",
    "    (yte >= 0) & (yte <= y_cap)\n",
    ")\n",
    "\n",
    "Xtr_clean, ytr_clean = Xtr.loc[mask_tr], ytr.loc[mask_tr]\n",
    "Xte_clean, yte_clean = Xte.loc[mask_te], yte.loc[mask_te]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b2ef1e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume y_cap was computed from ytr, e.g. y_cap = ytr.quantile(0.999)\n",
    "mask_tr = ((Xtr[\"trip_distance\"]>0) &\n",
    "           (Xtr[\"trip_duration_min\"]>0) & (Xtr[\"trip_duration_min\"]<=180) &\n",
    "           (ytr>=0) & (ytr<=y_cap))\n",
    "mask_te = ((Xte[\"trip_distance\"]>0) &\n",
    "           (Xte[\"trip_duration_min\"]>0) & (Xte[\"trip_duration_min\"]<=180) &\n",
    "           (yte>=0) & (yte<=y_cap))\n",
    "\n",
    "Xtr_c, ytr_c = Xtr.loc[mask_tr].copy(), ytr.loc[mask_tr].copy()\n",
    "Xte_c, yte_c = Xte.loc[mask_te].copy(), yte.loc[mask_te].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b39dd35e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid's rmse: 2.6478\n",
      "[200]\tvalid's rmse: 2.58561\n",
      "[300]\tvalid's rmse: 2.56948\n",
      "[400]\tvalid's rmse: 2.56854\n",
      "[500]\tvalid's rmse: 2.56437\n",
      "[600]\tvalid's rmse: 2.56342\n",
      "[700]\tvalid's rmse: 2.56295\n",
      "[800]\tvalid's rmse: 2.56277\n",
      "Early stopping, best iteration is:\n",
      "[741]\tvalid's rmse: 2.56175\n",
      "MAE: 1.7695322221889778\n",
      "RMSE: 3.8811028946485253\n",
      "R2 : 0.9486536236439578\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, numpy as np, lightgbm as lgb\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "NUM = [\"passenger_count\",\"trip_distance\",\"extra\",\"trip_duration_min\",\"pickup_hour\",\"pickup_dow\"]\n",
    "CAT = [\"RatecodeID\",\"PULocationID\",\"DOLocationID\",\"payment_type\"]\n",
    "FEATS = NUM + CAT\n",
    "\n",
    "# cast/align categories\n",
    "for c in CAT:\n",
    "    Xtr_c[c] = Xtr_c[c].astype(\"category\")\n",
    "    Xte_c[c] = Xte_c[c].astype(\"category\").cat.set_categories(Xtr_c[c].cat.categories)\n",
    "for c in NUM:\n",
    "    Xtr_c[c] = pd.to_numeric(Xtr_c[c], errors=\"coerce\")\n",
    "    Xte_c[c] = pd.to_numeric(Xte_c[c], errors=\"coerce\")\n",
    "\n",
    "# small train/valid split from TRAIN ONLY\n",
    "N_TR, N_VA = 1_000_000, 300_000\n",
    "chunk = Xtr_c.sample(min(N_TR+N_VA, len(Xtr_c)), random_state=42)\n",
    "tr_idx = chunk.sample(min(N_TR, len(chunk)), random_state=42).index\n",
    "va_idx = chunk.drop(tr_idx).index\n",
    "\n",
    "dtr = lgb.Dataset(Xtr_c.loc[tr_idx, FEATS], label=ytr_c.loc[tr_idx], categorical_feature=CAT)\n",
    "dva = lgb.Dataset(Xtr_c.loc[va_idx, FEATS], label=ytr_c.loc[va_idx], categorical_feature=CAT)\n",
    "\n",
    "params = dict(objective=\"regression\", metric=\"rmse\",\n",
    "              learning_rate=0.05, num_leaves=64,\n",
    "              feature_fraction=0.8, bagging_fraction=0.8, bagging_freq=1,\n",
    "              min_data_in_leaf=100, verbose=-1)\n",
    "\n",
    "gbm = lgb.train(params, dtr, num_boost_round=5000,\n",
    "                valid_sets=[dva], valid_names=[\"valid\"],\n",
    "                callbacks=[lgb.early_stopping(100), lgb.log_evaluation(100)])\n",
    "\n",
    "# test metrics\n",
    "def rmse_safe(y, p):\n",
    "    try: return mean_squared_error(y, p, squared=False)\n",
    "    except TypeError: return (mean_squared_error(y, p))**0.5\n",
    "\n",
    "pred = gbm.predict(Xte_c[FEATS], num_iteration=gbm.best_iteration)\n",
    "print(\"MAE:\", mean_absolute_error(yte_c, pred))\n",
    "print(\"RMSE:\", rmse_safe(yte_c, pred))\n",
    "print(\"R2 :\",  r2_score(yte_c, pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bc6c63fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: models/lightgbm_total_amount_1758450341.txt\n"
     ]
    }
   ],
   "source": [
    "import pathlib, joblib, time\n",
    "pathlib.Path(\"models\").mkdir(exist_ok=True)\n",
    "model_path = f\"models/lightgbm_total_amount_{int(time.time())}.txt\"\n",
    "gbm.save_model(model_path, num_iteration=gbm.best_iteration)\n",
    "\n",
    "meta = {\n",
    "    \"best_iter\": int(gbm.best_iteration),\n",
    "    \"metrics\": {\"MAE\": float(mae), \"RMSE\": float(rmse), \"R2\": float(r2)},\n",
    "    \"features\": FEATS,\n",
    "    \"cat_levels\": {c: list(Xtr_c[c].cat.categories) for c in CAT} if 'Xtr_c' in globals() else {c: list(Xtr[c].cat.categories) for c in CAT}\n",
    "}\n",
    "joblib.dump(meta, \"models/lightgbm_meta.joblib\")\n",
    "print(\"Saved:\", model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "291cedf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Data shapes (cleaned):\n",
      "Xtr_c: (27184185, 10) | Xte_c: (11580608, 10) \n",
      "\n",
      "=== Ridge(alpha=1.0) ===\n",
      "Fit time: 96.5s\n",
      "MAE:  3.3040\n",
      "RMSE: 6.2279\n",
      "R2:   0.8678\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  8.3min\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed: 39.9min\n",
      "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed: 85.2min finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    7.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:   33.1s\n",
      "[Parallel(n_jobs=8)]: Done 400 out of 400 | elapsed:  1.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== RandomForest (400 trees, depth=16, leaf=10, max_features=0.5; ~2M train) ===\n",
      "Fit time: 5177.0s | OOB R2: 0.9706\n",
      "MAE:  1.8432\n",
      "RMSE: 3.9636\n",
      "R2:   0.9464\n",
      "\n",
      "=== LightGBM (best_iter=741) ===\n",
      "MAE:  1.7695\n",
      "RMSE: 3.8811\n",
      "R2:   0.9487\n",
      "\n",
      "=== Summary (sorted by R2) ===\n",
      "                                         model      MAE     RMSE       R2  fit_s   OOB_R2\n",
      "                       LightGBM(best_iter=741) 1.769532 3.881103 0.948654    NaN      NaN\n",
      "RandomForest(400,d16,leaf10,mf=0.5; ~2M train) 1.843154 3.963584 0.946448 5177.0 0.970571\n",
      "                              Ridge(alpha=1.0) 3.303989 6.227938 0.867783   96.5      NaN\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>MAE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>R2</th>\n",
       "      <th>fit_s</th>\n",
       "      <th>OOB_R2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LightGBM(best_iter=741)</td>\n",
       "      <td>1.769532</td>\n",
       "      <td>3.881103</td>\n",
       "      <td>0.948654</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RandomForest(400,d16,leaf10,mf=0.5; ~2M train)</td>\n",
       "      <td>1.843154</td>\n",
       "      <td>3.963584</td>\n",
       "      <td>0.946448</td>\n",
       "      <td>5177.0</td>\n",
       "      <td>0.970571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ridge(alpha=1.0)</td>\n",
       "      <td>3.303989</td>\n",
       "      <td>6.227938</td>\n",
       "      <td>0.867783</td>\n",
       "      <td>96.5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            model       MAE      RMSE  \\\n",
       "2                         LightGBM(best_iter=741)  1.769532  3.881103   \n",
       "1  RandomForest(400,d16,leaf10,mf=0.5; ~2M train)  1.843154  3.963584   \n",
       "0                                Ridge(alpha=1.0)  3.303989  6.227938   \n",
       "\n",
       "         R2   fit_s    OOB_R2  \n",
       "2  0.948654     NaN       NaN  \n",
       "1  0.946448  5177.0  0.970571  \n",
       "0  0.867783    96.5       NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ==== Ridge + RandomForest (fast) on CLEANED data, with printed results ====\n",
    "# Requires: Xtr_c, ytr_c, Xte_c, yte_c already built; NUM, CAT defined.\n",
    "\n",
    "import time, numpy as np, pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# --- features ---\n",
    "NUM = [\"passenger_count\",\"trip_distance\",\"extra\",\"trip_duration_min\",\"pickup_hour\",\"pickup_dow\"]\n",
    "CAT = [\"RatecodeID\",\"PULocationID\",\"DOLocationID\",\"payment_type\"]\n",
    "FEATS = NUM + CAT\n",
    "\n",
    "# version-safe RMSE\n",
    "def rmse_safe(y_true, y_pred):\n",
    "    try:\n",
    "        return mean_squared_error(y_true, y_pred, squared=False)\n",
    "    except TypeError:\n",
    "        return (mean_squared_error(y_true, y_pred))**0.5\n",
    "\n",
    "# unified preprocessor (no leakage)\n",
    "pre = ColumnTransformer([\n",
    "    (\"num\", SimpleImputer(strategy=\"median\"), NUM),\n",
    "    (\"cat\", Pipeline([\n",
    "        (\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"oh\",  OneHotEncoder(handle_unknown=\"ignore\", min_frequency=50))\n",
    "    ]), CAT),\n",
    "], sparse_threshold=1.0)\n",
    "\n",
    "results = []\n",
    "\n",
    "print(\">>> Data shapes (cleaned):\")\n",
    "print(\"Xtr_c:\", Xtr_c[FEATS].shape, \"| Xte_c:\", Xte_c[FEATS].shape, \"\\n\")\n",
    "\n",
    "# ---- 1) Ridge baseline ----\n",
    "ridge = Pipeline([(\"pre\", pre), (\"est\", Ridge(alpha=1.0, random_state=42))])\n",
    "t0 = time.perf_counter()\n",
    "ridge.fit(Xtr_c[FEATS], ytr_c)\n",
    "t_ridge = time.perf_counter() - t0\n",
    "pred_r = ridge.predict(Xte_c[FEATS])\n",
    "\n",
    "ridge_mae  = mean_absolute_error(yte_c, pred_r)\n",
    "ridge_rmse = rmse_safe(yte_c, pred_r)\n",
    "ridge_r2   = r2_score(yte_c, pred_r)\n",
    "\n",
    "print(\"=== Ridge(alpha=1.0) ===\")\n",
    "print(f\"Fit time: {t_ridge:.1f}s\")\n",
    "print(f\"MAE:  {ridge_mae:.4f}\")\n",
    "print(f\"RMSE: {ridge_rmse:.4f}\")\n",
    "print(f\"R2:   {ridge_r2:.4f}\\n\")\n",
    "\n",
    "results.append(dict(\n",
    "    model=\"Ridge(alpha=1.0)\",\n",
    "    MAE=ridge_mae, RMSE=ridge_rmse, R2=ridge_r2, fit_s=round(t_ridge,1)\n",
    "))\n",
    "\n",
    "# ---- 2) Random Forest (fast, subsampled) ----\n",
    "N_TRAIN_RF = min(2_000_000, len(Xtr_c))  # adjust down if you want it faster\n",
    "idx_rf = Xtr_c.sample(n=N_TRAIN_RF, random_state=42).index\n",
    "\n",
    "rf = Pipeline([\n",
    "    (\"pre\", pre),\n",
    "    (\"est\", RandomForestRegressor(\n",
    "        n_estimators=400, max_depth=16, min_samples_leaf=10, max_features=0.5,\n",
    "        n_jobs=-1, random_state=42, oob_score=True, verbose=1\n",
    "    ))\n",
    "])\n",
    "t0 = time.perf_counter()\n",
    "rf.fit(Xtr_c.loc[idx_rf, FEATS], ytr_c.loc[idx_rf])\n",
    "t_rf = time.perf_counter() - t0\n",
    "pred_rf = rf.predict(Xte_c[FEATS])\n",
    "\n",
    "rf_mae  = mean_absolute_error(yte_c, pred_rf)\n",
    "rf_rmse = rmse_safe(yte_c, pred_rf)\n",
    "rf_r2   = r2_score(yte_c, pred_rf)\n",
    "rf_oob  = rf.named_steps[\"est\"].oob_score_\n",
    "\n",
    "print(\"=== RandomForest (400 trees, depth=16, leaf=10, max_features=0.5; ~2M train) ===\")\n",
    "print(f\"Fit time: {t_rf:.1f}s | OOB R2: {rf_oob:.4f}\")\n",
    "print(f\"MAE:  {rf_mae:.4f}\")\n",
    "print(f\"RMSE: {rf_rmse:.4f}\")\n",
    "print(f\"R2:   {rf_r2:.4f}\\n\")\n",
    "\n",
    "results.append(dict(\n",
    "    model=\"RandomForest(400,d16,leaf10,mf=0.5; ~2M train)\",\n",
    "    MAE=rf_mae, RMSE=rf_rmse, R2=rf_r2, OOB_R2=rf_oob, fit_s=round(t_rf,1)\n",
    "))\n",
    "\n",
    "# ---- 3) (optional) LightGBM row if you've trained `gbm` already ----\n",
    "try:\n",
    "    import lightgbm as lgb  # just to confirm availability\n",
    "    # ensure categories align for test (harmless if already aligned)\n",
    "    for c in CAT:\n",
    "        if Xtr_c[c].dtype.name != \"category\":\n",
    "            Xtr_c[c] = Xtr_c[c].astype(\"category\")\n",
    "        Xte_c[c] = Xte_c[c].astype(\"category\").cat.set_categories(Xtr_c[c].cat.categories)\n",
    "\n",
    "    pred_lgb = gbm.predict(Xte_c[FEATS], num_iteration=getattr(gbm, \"best_iteration\", None))\n",
    "    lgb_mae  = mean_absolute_error(yte_c, pred_lgb)\n",
    "    lgb_rmse = rmse_safe(yte_c, pred_lgb)\n",
    "    lgb_r2   = r2_score(yte_c, pred_lgb)\n",
    "\n",
    "    print(f\"=== LightGBM (best_iter={getattr(gbm,'best_iteration', 'NA')}) ===\")\n",
    "    print(f\"MAE:  {lgb_mae:.4f}\")\n",
    "    print(f\"RMSE: {lgb_rmse:.4f}\")\n",
    "    print(f\"R2:   {lgb_r2:.4f}\\n\")\n",
    "\n",
    "    results.append(dict(\n",
    "        model=f\"LightGBM(best_iter={getattr(gbm,'best_iteration','NA')})\",\n",
    "        MAE=lgb_mae, RMSE=lgb_rmse, R2=lgb_r2, fit_s=np.nan\n",
    "    ))\n",
    "except Exception as _e:\n",
    "    print(\"(LightGBM row skipped — no `gbm` in memory or prediction failed.)\\n\")\n",
    "\n",
    "# ---- Summary table ----\n",
    "res_df = pd.DataFrame(results).sort_values(\"R2\", ascending=False)\n",
    "print(\"=== Summary (sorted by R2) ===\")\n",
    "print(res_df.to_string(index=False))\n",
    "try:\n",
    "    display(res_df)  # pretty view in notebooks\n",
    "except Exception:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc8f9cd",
   "metadata": {},
   "source": [
    "1. LightGBM\n",
    "2. Random Forest\n",
    "3. Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16c3412",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------\n",
    "                                            Ignore the bottom                                                   \n",
    "----------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "146e0382",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27759268, 11896830)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TARGET = \"total_amount\"\n",
    "NUM = [\"passenger_count\",\"trip_distance\",\"extra\",\"trip_duration_min\",\"pickup_hour\",\"pickup_dow\"]\n",
    "CAT = [\"RatecodeID\",\"PULocationID\",\"DOLocationID\",\"payment_type\"]\n",
    "\n",
    "ms = m.sort_values(\"tpep_pickup_datetime\")\n",
    "cut = int(len(ms)*0.70)\n",
    "train, test = ms.iloc[:cut], ms.iloc[cut:]\n",
    "\n",
    "Xtr, ytr = train[NUM + CAT], train[TARGET].astype(\"float32\")\n",
    "Xte, yte = test [NUM + CAT], test [TARGET].astype(\"float32\")\n",
    "\n",
    "len(train), len(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "195bce3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all present: True\n",
      "pickup dtype: datetime64[ns, UTC] | dropoff dtype: datetime64[ns, UTC]\n",
      "duration dtype: float64 | hour dtype: Int8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tpep_pickup_datetime     0\n",
       "tpep_dropoff_datetime    0\n",
       "trip_duration_min        0\n",
       "pickup_hour              0\n",
       "pickup_dow               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.api.types import is_datetime64_any_dtype as is_dt\n",
    "\n",
    "# required columns present?\n",
    "req = [\"tpep_pickup_datetime\",\"tpep_dropoff_datetime\",\n",
    "       \"trip_duration_min\",\"pickup_hour\",\"pickup_dow\"]\n",
    "print(\"all present:\", set(req).issubset(m.columns))\n",
    "\n",
    "# dtypes look right?\n",
    "print(\"pickup dtype:\", m[\"tpep_pickup_datetime\"].dtype, \"| dropoff dtype:\", m[\"tpep_dropoff_datetime\"].dtype)\n",
    "print(\"duration dtype:\", m[\"trip_duration_min\"].dtype, \"| hour dtype:\", m[\"pickup_hour\"].dtype)\n",
    "\n",
    "# NAs\n",
    "m[req].isna().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47749114",
   "metadata": {},
   "source": [
    "Time-based split (avoid leakage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6fda7752",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27759268, 11896830)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TARGET = \"total_amount\"\n",
    "NUM = [\"passenger_count\",\"trip_distance\",\"extra\",\"trip_duration_min\",\"pickup_hour\",\"pickup_dow\"]\n",
    "CAT = [\"RatecodeID\",\"PULocationID\",\"DOLocationID\",\"payment_type\"]\n",
    "\n",
    "ms = m.sort_values(\"tpep_pickup_datetime\")\n",
    "cut = int(len(ms)*0.70)\n",
    "train, test = ms.iloc[:cut], ms.iloc[cut:]\n",
    "\n",
    "Xtr, ytr = train[NUM + CAT], train[TARGET].astype(\"float32\")\n",
    "Xte, yte = test [NUM + CAT], test [TARGET].astype(\"float32\")\n",
    "\n",
    "len(train), len(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "50397a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our valid RMSE: 5.885585734779989\n",
      "LGBM valid RMSE: 5.885585734779989\n",
      "Shapes (pred, yte): (11896830,) (11896830,)\n",
      "pred min/median/max: -86.56216676212419 16.04786345244758 292.2178636107599\n",
      "yte  min/median/max: -1301.8499755859375 15.960000038146973 401095.625\n",
      "NaNs (pred, yte): 0 0\n",
      "MAE: 1.732025154858528\n",
      "RMSE: 116.7277107813011\n",
      "R2: 0.020595296317712064\n"
     ]
    }
   ],
   "source": [
    "# 1) Check that our own RMSE on the VALIDATION subset matches LightGBM's best_score\n",
    "val_pred = gbm.predict(Xtr.loc[idx_va, FEATS], num_iteration=gbm.best_iteration)\n",
    "print(\"Our valid RMSE:\", rmse_safe(ytr.loc[idx_va], val_pred))\n",
    "print(\"LGBM valid RMSE:\", gbm.best_score[\"valid\"][\"rmse\"])\n",
    "\n",
    "# 2) Predict on TEST and inspect shapes and ranges\n",
    "pred = gbm.predict(Xte[FEATS], num_iteration=gbm.best_iteration)\n",
    "print(\"Shapes (pred, yte):\", pred.shape, yte.shape)\n",
    "print(\"pred min/median/max:\", float(np.nanmin(pred)), float(np.nanmedian(pred)), float(np.nanmax(pred)))\n",
    "print(\"yte  min/median/max:\", float(np.nanmin(yte)),  float(np.nanmedian(yte)),  float(np.nanmax(yte)))\n",
    "print(\"NaNs (pred, yte):\", int(np.isnan(pred).sum()), int(pd.isna(yte).sum()))\n",
    "\n",
    "# 3) Final metrics on TEST\n",
    "print(\"MAE:\", mean_absolute_error(yte, pred))\n",
    "print(\"RMSE:\", rmse_safe(yte, pred))\n",
    "print(\"R2:\",  r2_score(yte, pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8615fe",
   "metadata": {},
   "source": [
    "* Splitting by time to match how the model will be used in real lift: trained on the past, predicts the future.\n",
    "\n",
    "* A random split can quietly inflate scores for time-dependent data like taxi trips\n",
    "\n",
    "Why time-based split?\n",
    "* Realistic evaluation: Testing on later periods simulates that\n",
    "* Avoid look-ahead leakage: Global steps (impute medians, scaling, ont-hot categories) must be learned on train only. If future rows is mix into training data, future information might be leak\n",
    "* Handles temporal correlation: Trips close in time share weather, traffic, events, policy changes. Random splits sprinkle near-identical conditions across train/test -> over-optimistic metrics\n",
    "* Detects drift: Fare/behavior shift by month/season. A chronological split shows how well the moel generalizes to a new regime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f7cf1df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn: 1.7.2\n",
      "python: /Users/chrisnjw/.pyenv/versions/myproj-311/bin/python\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Signature (y_true, y_pred, *, sample_weight=None, multioutput='uniform_average')>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn, sys, inspect\n",
    "print(\"sklearn:\", sklearn.__version__)\n",
    "print(\"python:\", sys.executable)\n",
    "inspect.signature(__import__(\"sklearn.metrics\").metrics.mean_squared_error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "362c5d50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6.399168288871747, np.float64(55.5506942344371), 0.07257096419322462)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "mae = mean_absolute_error(yte, pred)\n",
    "\n",
    "try:\n",
    "    rmse = mean_squared_error(yte, pred, squared=False)  # new API\n",
    "except TypeError:\n",
    "    rmse = np.sqrt(mean_squared_error(yte, pred))        # fallback for old sklearn\n",
    "\n",
    "r2  = r2_score(yte, pred)\n",
    "(mae, rmse, r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2de5df6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6.399168288871747,\n",
       " np.float64(55.5506942344371),\n",
       " 0.07257096419322462,\n",
       " 11.60715389251709,\n",
       " np.float64(57.711281748161035))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "baseline = np.full_like(yte, fill_value=float(ytr.mean()))\n",
    "b_mae = mean_absolute_error(yte, baseline)\n",
    "try:\n",
    "    b_rmse = mean_squared_error(yte, baseline, squared=False)\n",
    "except TypeError:\n",
    "    b_rmse = np.sqrt(mean_squared_error(yte, baseline))\n",
    "\n",
    "mae, rmse, r2, b_mae, b_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a149cd42",
   "metadata": {},
   "source": [
    "Baseline (Ridge) with proper preprocessing in a pipleline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9666c640",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.8779829572318527, np.float64(6.585973441997223), 0.8788138401564854)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "TARGET = \"total_amount\"\n",
    "NUM = [\"passenger_count\",\"trip_distance\",\"extra\",\"trip_duration_min\",\"pickup_hour\",\"pickup_dow\"]\n",
    "CAT = [\"RatecodeID\",\"PULocationID\",\"DOLocationID\",\"payment_type\"]\n",
    "\n",
    "Xtr, ytr = train[NUM+CAT], train[TARGET].astype(\"float32\")\n",
    "Xte, yte = test [NUM+CAT], test [TARGET].astype(\"float32\")\n",
    "\n",
    "pre_tree = ColumnTransformer([\n",
    "    (\"num\", SimpleImputer(strategy=\"median\"), NUM),\n",
    "    (\"cat\", Pipeline([\n",
    "        (\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"oh\",  OneHotEncoder(handle_unknown=\"ignore\", min_frequency=5))  # collapse rare cats\n",
    "    ]), CAT),\n",
    "])\n",
    "\n",
    "rf = Pipeline([\n",
    "    (\"pre\", pre_tree),\n",
    "    (\"est\", RandomForestRegressor(\n",
    "        n_estimators=300,\n",
    "        max_depth=20,            # cap depth for speed/overfit control\n",
    "        min_samples_leaf=5,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# warm up on a sample; then remove these two lines for full fit\n",
    "idx = Xtr.sample(n=1_000_000, random_state=42).index\n",
    "rf.fit(Xtr.loc[idx], ytr.loc[idx])\n",
    "\n",
    "# quick eval on a slice; then remove head(...) for full eval\n",
    "pred = rf.predict(Xte.head(500_000))\n",
    "\n",
    "# metrics (robust to sklearn version)\n",
    "mae = mean_absolute_error(yte.head(500_000), pred)\n",
    "try:\n",
    "    rmse = mean_squared_error(yte.head(500_000), pred, squared=False)\n",
    "except TypeError:\n",
    "    rmse = np.sqrt(mean_squared_error(yte.head(500_000), pred))\n",
    "r2 = r2_score(yte.head(500_000), pred)\n",
    "(mae, rmse, r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56941edb",
   "metadata": {},
   "source": [
    "y = dollar fare I am predicting\n",
    "X = numeric + categorical predictors based on my time-based split\n",
    "\n",
    "* Train 300 trees, each grown (up to depth 20) on the 1M training rows after OHE \n",
    "\n",
    "MAE = 1.88 dollars\n",
    "RMSE = 6.59\n",
    "R^2 = 0.879"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791603e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stockulator",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
